{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\diego\\OneDrive\\Documentos\\GitHub\\data_analytics_zhaw\\Week_11\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The squirrel runs up the tree. Today i had an apple for breakfast. Both squirrels and apples live in trees. Squirrels dont eat apples as breakfast, but rather nuts'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentences)\n",
    "d1 = 'The squirrel runs up the tree.'\n",
    "d2 = 'Today i had an apple for breakfast.'\n",
    "d3 = 'Both squirrels and apples live in trees.'\n",
    "d4 = 'Squirrels dont eat apples as breakfast, but rather nuts'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3 + ' ' + d4\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the squirrel runs up the tree. today i had an apple for breakfast. both squirrels and apples live in trees. squirrels dont eat apples as breakfast, but rather nuts'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the squirrel runs up the tree today i had an apple for breakfast both squirrels and apples live in trees squirrels dont eat apples as breakfast but rather nuts'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'for', 'mustn', 'that', 'to', 'this', 'or', 'no', 'hers', \"you're\", \"you'll\", 'how', 'weren', 'over', 'ourselves', 'more', 'mightn', 'being', 'too', 'now', \"hadn't\", 'own', 'her', 'any', \"mustn't\", 'him', 'below', 'few', 'am', 'about', 'not', 'd', 'hadn', \"wasn't\", 'again', \"you'd\", 'does', 'won', \"wouldn't\", 'are', 'from', 'when', 'where', 'and', 'of', 'in', 'very', \"needn't\", 'ain', 'yours', 'couldn', 'shan', 'on', 'during', 'an', 'hasn', 'my', 'm', 'why', 'before', 'aren', \"isn't\", 'he', 'those', 'these', 'their', \"doesn't\", 'doing', 'most', \"weren't\", 'between', 'we', \"mightn't\", 'off', 'who', 'themselves', 'be', 'yourself', 'into', 'by', 'ours', 'been', 've', 'were', 'such', 'each', 'further', 'myself', 'had', 'up', 'having', 'have', 'same', 'some', 'yourselves', \"couldn't\", 'until', 'do', \"aren't\", 'can', 'if', 'under', \"should've\", 'both', \"hasn't\", 'after', 's', 'while', 'only', \"she's\", 'its', 'me', 'so', 'a', 'with', 'them', 're', \"you've\", 'then', 'nor', 'needn', 'was', 'itself', 'will', 'doesn', 'than', 'our', 'it', 'through', 'his', 'haven', 'because', \"don't\", 'isn', 'against', 'i', 'should', 'y', 'which', 'but', \"it's\", 'once', 'll', 'above', \"haven't\", 'theirs', 'you', 'down', \"shan't\", 'himself', 'did', 'didn', 'your', 'there', \"shouldn't\", 'all', 'whom', 'wasn', 'just', 'wouldn', 'at', 'shouldn', 'the', 'has', 'as', \"that'll\", \"won't\", 'they', 'other', 't', 'is', \"didn't\", 'herself', 'don', 'out', 'what', 'ma', 'she', 'o', 'here'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['squirrel', 'runs', 'tree', 'today', 'apple', 'breakfast', 'squirrels', 'apples', 'live', 'trees', 'squirrels', 'dont', 'eat', 'apples', 'breakfast', 'rather', 'nuts']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['squirrel', 'runs', 'tree', 'today', 'apple', 'breakfast', 'squirrels', 'apples', 'live', 'trees', 'squirrels', 'dont', 'eat', 'apples', 'breakfast', 'rather', 'nuts'] \n",
      "\n",
      "After lemmatization:\n",
      "['squirrel', 'run', 'tree', 'today', 'apple', 'breakfast', 'squirrels', 'apples', 'live', 'tree', 'squirrels', 'dont', 'eat', 'apples', 'breakfast', 'rather', 'nut']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "corpus = ['squirrel run tree', \n",
    "          'today apple breakfast', \n",
    "          'squirrels apples live tree',\n",
    "          'squirrels dont eat apples breakfast rather nut']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   apple  apples  breakfast  dont  eat  live  nut  rather  run  squirrel  \\\n",
      "0      0       0          0     0    0     0    0       0    1         1   \n",
      "1      1       0          1     0    0     0    0       0    0         0   \n",
      "2      0       1          0     0    0     1    0       0    0         0   \n",
      "3      0       1          1     1    1     0    1       1    0         0   \n",
      "\n",
      "   squirrels  today  tree  \n",
      "0          0      0     1  \n",
      "1          0      1     0  \n",
      "2          1      0     1  \n",
      "3          1      0     0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   apple breakfast  apples breakfast  apples live  breakfast rather  dont eat  \\\n",
      "0                0                 0            0                 0         0   \n",
      "1                1                 0            0                 0         0   \n",
      "2                0                 0            1                 0         0   \n",
      "3                0                 1            0                 1         1   \n",
      "\n",
      "   eat apples  live tree  rather nut  run tree  squirrel run  \\\n",
      "0           0          0           0         1             1   \n",
      "1           0          0           0         0             0   \n",
      "2           0          1           0         0             0   \n",
      "3           1          0           1         0             0   \n",
      "\n",
      "   squirrels apples  squirrels dont  today apple  \n",
      "0                 0               0            0  \n",
      "1                 0               0            1  \n",
      "2                 1               0            0  \n",
      "3                 0               1            0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 13 \n",
      "\n",
      "The words in the corpus: \n",
      " {'apple', 'nut', 'run', 'tree', 'squirrel', 'eat', 'live', 'today', 'dont', 'squirrels', 'breakfast', 'apples', 'rather'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "    apple     nut     run    tree  squirrel     eat  live   today    dont  \\\n",
      "0  0.0000  0.0000  0.3333  0.3333    0.3333  0.0000  0.00  0.0000  0.0000   \n",
      "1  0.3333  0.0000  0.0000  0.0000    0.0000  0.0000  0.00  0.3333  0.0000   \n",
      "2  0.0000  0.0000  0.0000  0.2500    0.0000  0.0000  0.25  0.0000  0.0000   \n",
      "3  0.0000  0.1429  0.0000  0.0000    0.0000  0.1429  0.00  0.0000  0.1429   \n",
      "\n",
      "   squirrels  breakfast  apples  rather  \n",
      "0     0.0000     0.0000  0.0000  0.0000  \n",
      "1     0.0000     0.3333  0.0000  0.0000  \n",
      "2     0.2500     0.0000  0.2500  0.0000  \n",
      "3     0.1429     0.1429  0.1429  0.1429  \n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "          apple:     0.6021\n",
      "            nut:     0.6021\n",
      "            run:     0.6021\n",
      "           tree:      0.301\n",
      "       squirrel:     0.6021\n",
      "            eat:     0.6021\n",
      "           live:     0.6021\n",
      "          today:     0.6021\n",
      "           dont:     0.6021\n",
      "      squirrels:      0.301\n",
      "      breakfast:      0.301\n",
      "         apples:      0.301\n",
      "         rather:     0.6021\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "    apple    nut     run    tree  squirrel    eat    live   today   dont  \\\n",
      "0  0.0000  0.000  0.2007  0.1003    0.2007  0.000  0.0000  0.0000  0.000   \n",
      "1  0.2007  0.000  0.0000  0.0000    0.0000  0.000  0.0000  0.2007  0.000   \n",
      "2  0.0000  0.000  0.0000  0.0752    0.0000  0.000  0.1505  0.0000  0.000   \n",
      "3  0.0000  0.086  0.0000  0.0000    0.0000  0.086  0.0000  0.0000  0.086   \n",
      "\n",
      "   squirrels  breakfast  apples  rather  \n",
      "0     0.0000     0.0000  0.0000   0.000  \n",
      "1     0.0000     0.1003  0.0000   0.000  \n",
      "2     0.0752     0.0000  0.0752   0.000  \n",
      "3     0.0430     0.0430  0.0430   0.086  \n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Political', 'JJ', 'O'),\n",
      " ('forces', 'NNS', 'O'),\n",
      " ('keep', 'VB', 'O'),\n",
      " ('avoiding', 'VBG', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('subject', 'NN', 'I-NP'),\n",
      " ('when', 'WRB', 'O'),\n",
      " ('questioned', 'VBN', 'O'),\n",
      " ('about', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('severity', 'NN', 'I-NP'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('incident', 'NN', 'I-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('how', 'WRB', 'O'),\n",
      " ('it', 'PRP', 'O'),\n",
      " ('may', 'MD', 'O'),\n",
      " ('affect', 'VB', 'O'),\n",
      " ('GDP', 'NNP', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('other', 'JJ', 'O'),\n",
      " ('financial', 'JJ', 'O'),\n",
      " ('indicators', 'NNS', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('near', 'JJ', 'I-NP'),\n",
      " ('future', 'NN', 'I-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = '''Political forces keep avoiding the subject when questioned about\n",
    "the severity of the incident and how it may affect GDP and other \n",
    "financial indicators in the near future.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff4604",
   "metadata": {},
   "source": [
    "The POS tags results for the first five words are as follows:\n",
    "Political -> JJ, which means this word is classified as an adjective\n",
    "forces -> NNS, which classifies this word as a plural noun\n",
    "keep -> VB, thus the word is considered to be a verb in base form \n",
    "avoiding -> VBG, this represents a verb in gerund form (present continuous)\n",
    "the -> DT, meaning the word 'the' is called a determiner (article the)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "NT\n",
      "Windows | 10\n",
      "Datetime: 2024-12-10 07:39:37\n",
      "Python Version: 3.10.5\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
